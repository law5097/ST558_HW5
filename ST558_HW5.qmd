---
title: "ST558 HW5 - Lee Worthington"
format: html
editor: visual
---

## Task 1: conceptual questions

### Question 1 - What is the purpose of using cross-validation when fitting a random forest model?

> Using CV with a random forest model allows you to:
>
> -   Tune the hyperparameters of the random forest model to find the "optimal" hyperparameters and prevent overfitting
> -   Estimate the performance of the model on unseen test data using the 1 of k folds as the test set

### Question 2 - Describe the bagged tree algorithm.

> A bagged tree model involves:
>
> -   Generating multiple datasets by sampling with replacement from the original dataset samples of size n
> -   Fitting a decision tree on each bootstrapped sample
> -   Generating a prediction with each tree
> -   Then generally taking the average or mode of the tree predictions in order to generate the final prediction

### Question 3 - What is meant by a general linear model?

> A generAL linear model is a linear model where the response is continuous and normal, that allows for both continuous and categorical predictors.

### Question 4 - When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?

> In MLR including an interaction term allows you to capture the combined effect of multiple predictors on the target variable.

> Basically it allows the model to capture situations where a predictors effect on the target depends on another predictor. For instance if you had a sex\*height interaction term for predicting weight the effect of height on weight may vary based on sex.

### Question 5 - Why do we split our data into a training and test set?

> The main purpose of a test set is to obtain an unbiased assessment of how a model generalizes to unseen data. To accomplish this we need to split our full dataset into a training set, which is used to fit and tune the model, and a completely separate test set containing unseen data on which we can measure the accuracy and performance of the trained model.

> This ensures that the evaluation metrics reflect the model's ability to perform on new, unseen data, rather than just the data it was trained on.

## Task 2: fitting models

### Question 1 - Understand the data

#### Load and summarize the data

```{r}
#| eval: true
#| warning: false

# load libraries
library(tidyverse)
library(GGally)
library(caret)

# set seed
set.seed(1)  

# Read in the data
heart_data_1 <- read_csv(
  'C:\\Users\\lawor\\OneDrive\\Desktop\\School\\ST 558\\Homework\\ST558_HW5\\heart.csv',
  show_col_types = FALSE
)

# Print summary
summary(heart_data_1)

# Print level count to confirm on these categorical/binary fields
heart_data_1 |> 
  summarise(
    Sex = length(unique(Sex)),
    ChestPainType = length(unique(ChestPainType)),
    FastingBS = length(unique(FastingBS)),
    RestingECG = length(unique(RestingECG)),
    ExerciseAngina = length(unique(ExerciseAngina)),
    ST_Slope = length(unique(ST_Slope)),
    HeartDisease = length(unique(HeartDisease))
  ) |>
  print()

```

> Base on the summary data (and eyeballing the csv):
>
> -   Age, RestingBP, Cholsterol, MaxHR, and Oldpeak are continuous
> -   ChestPainType, RestingECG, and ST_Slope are characters
> -   Sex, FastingBS, ExerciseAngina, and HeartDisease are binary

#### Count missing values with respect ot HeartDisease

```{r}
#| eval: true
#| warning: false

# Count missing values grouped by HeartDisease and print @@@@@@@@@@@@@@@@@@@@@
heart_data_1 |>
  group_by(HeartDisease) |>
  summarise_all(~sum(is.na(.))) |>
  gather(key = "Variable", value = "MissingValues", -HeartDisease) |>
  arrange(desc(MissingValues), HeartDisease) |>
  print()

```

> There appear to be no missing values at all, looking at the raw data confirms this as well

#### Plots focusing on HeartDisease

```{r}
#| eval: true
#| warning: false

# Generate pair plots in chunks so this is readable
GGally::ggpairs(heart_data_1, columns = c(12, 1, 2))
GGally::ggpairs(heart_data_1, columns = c(12, 3, 4))
GGally::ggpairs(heart_data_1, columns = c(12, 5, 6))
GGally::ggpairs(heart_data_1, columns = c(12, 7, 8))
GGally::ggpairs(heart_data_1, columns = c(12, 9, 10))
GGally::ggpairs(heart_data_1, columns = c(12, 11))
```

> Based on these plots in terms of impact on HeartDisease these variarbles seem to have a large effect:
>
> -   ChestPainType - Atypical Angina (ATA), etc, based on a quick google search this seems like it's probably relevant when predicting heart disease
> -   ExerciseAngina - Chest pain from exercise flag, again probably makes sense as a predictor
> -   ST_Slope - I believeve this is the slope on cardiograms? Assignment says to remove this in the next step though, don't use
> -   Sex - Is probably the next best predictor in terms of seperation

### Question 2 - Create new variables

```{r}
#| eval: true
#| warning: false

# Convert columns to appropriate data types
heart_data_2 <- heart_data_1 |>
  mutate(HeartDiseaseFactor = as.factor(HeartDisease)) |>
  select(-ST_Slope, -HeartDisease)

# Print output
head(heart_data_2, 5)

```

### Question 3 - Create dummy variables for KNN

```{r}
#| eval: true
#| warning: false

# Create dummy variables for categorical predictors
dummies <- dummyVars(~ Sex + ExerciseAngina + ChestPainType + RestingECG, data = heart_data_2)
dummy_data <- predict(dummies, newdata = heart_data_2)

# Convert dummy_data to a data frame
dummy_data <- as.data.frame(dummy_data)

# Combine the dummy variables with the original dataset
heart_data_2_dummies <- bind_cols(heart_data_2, dummy_data)

# Print the updated heart_data
head(heart_data_2_dummies, 5)

```

### Split the data into train/test for the dummy and non-dummy data

```{r}
#| eval: true
#| warning: false

# Split dummy data and drop original fields
train_index_dummies <- createDataPartition(heart_data_2_dummies$HeartDiseaseFactor, p = 0.8, list = FALSE)

train_data_dummies <- heart_data_2_dummies[train_index_dummies, ]
train_data_dummies <- train_data_dummies |> select(-Sex, -ExerciseAngina, -ChestPainType, -RestingECG)

test_data_dummies <- heart_data_2_dummies[-train_index_dummies, ]
test_data_dummies <- test_data_dummies |> select(-Sex, -ExerciseAngina, -ChestPainType, -RestingECG)

# Split dummy data and drop original fields
train_index <- createDataPartition(heart_data_2$HeartDiseaseFactor, p = 0.8, list = FALSE)
train_data <- heart_data_2[train_index, ]
test_data <- heart_data_2[-train_index, ]

# check results
nrow(train_data)
nrow(train_data_dummies)
nrow(test_data)
nrow(test_data_dummies)

```

### Fit KNN model with Caret

```{r}
#| eval: true
#| warning: false

# Fit KNN model
knn_model <- train(
  HeartDiseaseFactor ~ .,
  data = train_data_dummies,
  method = "knn",
  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3),
  tuneGrid = expand.grid(k = 1:40),
  preProcess = c("center", "scale")
)

# Fit summary
# summary(knn_model)

# Generate predictions
test_data_dummies$HeartDiseaseKNN <- predict(knn_model, newdata = test_data_dummies)

# Print confusion matrix
confusionMatrix(test_data_dummies$HeartDiseaseKNN, test_data_dummies$HeartDiseaseFactor) # accuracy 0.8087 

```
> KNN model here has an accuracy of .8306 on the test set

### Fit logistic regression model with Caret

```{r}
#| eval: true
#| warning: false

# Fit logistic model with every predictor
logistic_model_1 <- train(
  HeartDiseaseFactor ~ .,
  data = train_data,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3),
  preProcess = c("center", "scale")
)

# Fit logistic model with what appear to be strong predictors looking at EDA
logistic_model_2 <- train(
  HeartDiseaseFactor ~ .,
  data = train_data |> select(HeartDiseaseFactor, ChestPainType, ExerciseAngina, Sex),
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3),
  preProcess = c("center", "scale")
)

# Fit logistic model using ExerciseAgina as the single predictor
logistic_model_3 <- train(
  HeartDiseaseFactor ~ .,
  data = train_data |> select(HeartDiseaseFactor, ExerciseAngina),
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3),
  preProcess = c("center", "scale")
)

# Print the training fits for each
#logistic_model_1 # accuracy 0.8245502
#logistic_model_2 # accuracy 0.7950438
#logistic_model_3 # accuracy 0.7266481

# Print summary for best model
summary(logistic_model_1)

# Generate predictions on test data + print confusion matrix for the best model (model 1)
test_data$HeartDiseaseLogistic1 <- predict(logistic_model_1, newdata = test_data)
test_data$HeartDiseaseLogistic2 <- predict(logistic_model_2, newdata = test_data)
test_data$HeartDiseaseLogistic3 <- predict(logistic_model_3, newdata = test_data)
confusionMatrix(test_data$HeartDiseaseLogistic1, test_data$HeartDiseaseFactor) # accuracy 0.8033

```
> Best logistic model has an accuracy of 0.8033 on the test set

### Fit tree models with Caret

```{r}
#| eval: true
#| warning: false

# Fit classification tree model with every predictor
tree_model <- train(
  HeartDiseaseFactor ~ .,
  data = train_data,
  method = "rpart",
  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3),
  tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001)),
  preProcess = c("center", "scale")
)

# Fit random forest model with every predictor
random_forest_model <- train(
  HeartDiseaseFactor ~ .,
  data = train_data,
  method = "rf",
  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3),
  tuneGrid = expand.grid(mtry = 1:(ncol(train_data) - 1)),
  preProcess = c("center", "scale")
)

# Fit boosted model with every predictor
boosted_tree_model <- train(
  HeartDiseaseFactor ~ .,
  data = train_data,
  method = "gbm",
  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3),
  tuneGrid = expand.grid(
    n.trees = c(25, 50, 100, 200),
    interaction.depth = 1:3,
    shrinkage = 0.1,
    n.minobsinnode = 10
  ),
  verbose = FALSE,
  preProcess = c("center", "scale")
)

```

```{r}
#| eval: true
#| warning: false

# Generate predictions
test_data$HeartDiseaseTree <- predict(tree_model, newdata = test_data)
test_data$HeartDiseaseForest <- predict(random_forest_model, newdata = test_data)
test_data$HeartDiseaseBoosted <- predict(boosted_tree_model, newdata = test_data)

# Print confusion matrix
confusionMatrix(test_data$HeartDiseaseTree, test_data$HeartDiseaseFactor)
confusionMatrix(test_data$HeartDiseaseForest, test_data$HeartDiseaseFactor)
confusionMatrix(test_data$HeartDiseaseBoosted, test_data$HeartDiseaseFactor)

```
> Tree performance is weaker than the others, but the random forest and boosted tree have similar performance to the logistic model

```{r}
#| eval: true
#| warning: false

# Calculate and print accuracy for each model
accuracy_knn <- confusionMatrix(test_data_dummies$HeartDiseaseFactor, test_data_dummies$HeartDiseaseKNN)$overall['Accuracy']

accuracy_logistic1 <- confusionMatrix(test_data$HeartDiseaseLogistic1, test_data$HeartDiseaseFactor)$overall['Accuracy']
accuracy_logistic2 <- confusionMatrix(test_data$HeartDiseaseLogistic2, test_data$HeartDiseaseFactor)$overall['Accuracy']
accuracy_logistic3 <- confusionMatrix(test_data$HeartDiseaseLogistic3, test_data$HeartDiseaseFactor)$overall['Accuracy']

accuracy_tree <- confusionMatrix(test_data$HeartDiseaseTree, test_data$HeartDiseaseFactor)$overall['Accuracy']
accuracy_forest <- confusionMatrix(test_data$HeartDiseaseForest, test_data$HeartDiseaseFactor)$overall['Accuracy']
accuracy_boosted <- confusionMatrix(test_data$HeartDiseaseBoosted, test_data$HeartDiseaseFactor)$overall['Accuracy']


# Create a dataframe to store accuracy
accuracy_df <- data.frame(
  Model = c('KNN', 'Logistic1', 'Logistic2', 'Logistic3', 'Tree', 'RandomForest', 'BoostedTree'),
  Accuracy = c(accuracy_knn, accuracy_logistic1, accuracy_logistic2, accuracy_logistic3, accuracy_tree, accuracy_forest, accuracy_boosted)
)

# Print the accuracy comparison
print(accuracy_df)

```
> Overall the KNN model has the best test test accurracy as seen above, however the logistic model/random forest/boosted tree have almost comparable performance

