[
  {
    "objectID": "ST558_HW5.html",
    "href": "ST558_HW5.html",
    "title": "ST558 HW5 - Lee Worthington",
    "section": "",
    "text": "Using CV with a random forest model allows you to:\n\nTune the hyperparameters of the random forest model to find the “optimal” hyperparameters and prevent overfitting\nEstimate the performance of the model on unseen test data using the 1 of k folds as the test set\n\n\n\n\n\n\nA bagged tree model involves:\n\nGenerating multiple datasets by sampling with replacement from the original dataset samples of size n\nFitting a decision tree on each bootstrapped sample\nGenerating a prediction with each tree\nThen generally taking the average or mode of the tree predictions in order to generate the final prediction\n\n\n\n\n\n\nA generAL linear model is a linear model where the response is continuous and normal, that allows for both continuous and categorical predictors.\n\n\n\n\n\nIn MLR including an interaction term allows you to capture the combined effect of multiple predictors on the target variable.\n\n\nBasically it allows the model to capture situations where a predictors effect on the target depends on another predictor. For instance if you had a sex*height interaction term for predicting weight the effect of height on weight may vary based on sex.\n\n\n\n\n\nThe main purpose of a test set is to obtain an unbiased assessment of how a model generalizes to unseen data. To accomplish this, we need to split our full dataset into a training set, which is used to fit and tune the model, and a completely separate test set containing unseen data, on which we can measure the accuracy and performance of the trained model.\n\n\nThis ensures that the evaluation metrics reflect the model’s ability to perform on new, unseen data, rather than just the data it was trained on."
  },
  {
    "objectID": "ST558_HW5.html#task-2-fitting-models",
    "href": "ST558_HW5.html#task-2-fitting-models",
    "title": "ST558 HW5 - Lee Worthington",
    "section": "Task 2: fitting models",
    "text": "Task 2: fitting models\n\nQuestion 1 - Understand the data\n\nLoad and summarize the data\n\n# load libraries\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(caret)\n\n# Read in the data\nheart_data_1 <- read_csv(\n  'C:\\\\Users\\\\lawor\\\\OneDrive\\\\Desktop\\\\School\\\\ST 558\\\\Homework\\\\ST558_HW5\\\\heart.csv',\n  show_col_types = FALSE\n)\n\n# Print summary\nsummary(heart_data_1)\n\n      Age            Sex            ChestPainType        RestingBP    \n Min.   :28.00   Length:918         Length:918         Min.   :  0.0  \n 1st Qu.:47.00   Class :character   Class :character   1st Qu.:120.0  \n Median :54.00   Mode  :character   Mode  :character   Median :130.0  \n Mean   :53.51                                         Mean   :132.4  \n 3rd Qu.:60.00                                         3rd Qu.:140.0  \n Max.   :77.00                                         Max.   :200.0  \n  Cholesterol      FastingBS       RestingECG            MaxHR      \n Min.   :  0.0   Min.   :0.0000   Length:918         Min.   : 60.0  \n 1st Qu.:173.2   1st Qu.:0.0000   Class :character   1st Qu.:120.0  \n Median :223.0   Median :0.0000   Mode  :character   Median :138.0  \n Mean   :198.8   Mean   :0.2331                      Mean   :136.8  \n 3rd Qu.:267.0   3rd Qu.:0.0000                      3rd Qu.:156.0  \n Max.   :603.0   Max.   :1.0000                      Max.   :202.0  \n ExerciseAngina        Oldpeak          ST_Slope          HeartDisease   \n Length:918         Min.   :-2.6000   Length:918         Min.   :0.0000  \n Class :character   1st Qu.: 0.0000   Class :character   1st Qu.:0.0000  \n Mode  :character   Median : 0.6000   Mode  :character   Median :1.0000  \n                    Mean   : 0.8874                      Mean   :0.5534  \n                    3rd Qu.: 1.5000                      3rd Qu.:1.0000  \n                    Max.   : 6.2000                      Max.   :1.0000  \n\n# Print level count to confirm on these categorical/binary fields\nheart_data_1 |> \n  summarise(\n    Sex = length(unique(Sex)),\n    ChestPainType = length(unique(ChestPainType)),\n    FastingBS = length(unique(FastingBS)),\n    RestingECG = length(unique(RestingECG)),\n    ExerciseAngina = length(unique(ExerciseAngina)),\n    ST_Slope = length(unique(ST_Slope)),\n    HeartDisease = length(unique(HeartDisease))\n  ) |>\n  print()\n\n# A tibble: 1 × 7\n    Sex ChestPainType FastingBS RestingECG ExerciseAngina ST_Slope HeartDisease\n  <int>         <int>     <int>      <int>          <int>    <int>        <int>\n1     2             4         2          3              2        3            2\n\n\n\nBase on the summary data (and eyeballing the csv):\n\nAge, RestingBP, Cholsterol, MaxHR, and Oldpeak are continuous\nChestPainType, RestingECG, and ST_Slope are characters\nSex, FastingBS, ExerciseAngina, and HeartDisease are binary\n\n\n\n\nCount missing values with respect ot HeartDisease\n\n# Count missing values grouped by HeartDisease and print @@@@@@@@@@@@@@@@@@@@@\nheart_data_1 |>\n  group_by(HeartDisease) |>\n  summarise_all(~sum(is.na(.))) |>\n  gather(key = \"Variable\", value = \"MissingValues\", -HeartDisease) |>\n  arrange(desc(MissingValues), HeartDisease) |>\n  print()\n\n# A tibble: 22 × 3\n   HeartDisease Variable       MissingValues\n          <dbl> <chr>                  <int>\n 1            0 Age                        0\n 2            0 Sex                        0\n 3            0 ChestPainType              0\n 4            0 RestingBP                  0\n 5            0 Cholesterol                0\n 6            0 FastingBS                  0\n 7            0 RestingECG                 0\n 8            0 MaxHR                      0\n 9            0 ExerciseAngina             0\n10            0 Oldpeak                    0\n# ℹ 12 more rows\n\n\n\nThere appear to be no missing values at all, looking at the raw data confirms this as well\n\n\n\nPlots focusing on HeartDisease\n\n# Generate pair plots in chunks so this is readable\nGGally::ggpairs(heart_data_1, columns = c(12, 1, 2))\n\n\n\nGGally::ggpairs(heart_data_1, columns = c(12, 3, 4))\n\n\n\nGGally::ggpairs(heart_data_1, columns = c(12, 5, 6))\n\n\n\nGGally::ggpairs(heart_data_1, columns = c(12, 7, 8))\n\n\n\nGGally::ggpairs(heart_data_1, columns = c(12, 9, 10))\n\n\n\nGGally::ggpairs(heart_data_1, columns = c(12, 11))\n\n\n\n\n\nBased on these plots in terms of impact on HeartDisease these variarbles seem to have a large effect:\n\nChestPainType - Atypical Angina (ATA), etc, based on a quick google search this seems like it’s probably relevant when predicting heart disease\nExerciseAngina - Chest pain from exercise flag, again probably makes sense as a predictor\nST_Slope - I believeve this is the slope on cardiograms? Assignment says to remove this in the next step though, don’t use\nSex is probably the next best predictor in terms of seperation\n\n\n\n\n\nQuestion 2 - Create new variables\n\n# Convert columns to appropriate data types\nheart_data_2 <- heart_data_1 |>\n  mutate(HeartDiseaseFactor = as.factor(HeartDisease)) |>\n  select(-ST_Slope, -HeartDisease)\n\n# Print output\nhead(heart_data_2, 5)\n\n# A tibble: 5 × 11\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  <dbl> <chr> <chr>             <dbl>       <dbl>     <dbl> <chr>      <dbl>\n1    40 M     ATA                 140         289         0 Normal       172\n2    49 F     NAP                 160         180         0 Normal       156\n3    37 M     ATA                 130         283         0 ST            98\n4    48 F     ASY                 138         214         0 Normal       108\n5    54 M     NAP                 150         195         0 Normal       122\n# ℹ 3 more variables: ExerciseAngina <chr>, Oldpeak <dbl>,\n#   HeartDiseaseFactor <fct>\n\n\n\n\nQuestion 3 - Create dummy variables for KNN\n\n# Create dummy variables for categorical predictors\ndummies <- dummyVars(~ Sex + ExerciseAngina + ChestPainType + RestingECG, data = heart_data_2)\ndummy_data <- predict(dummies, newdata = heart_data_2)\n\n# Convert dummy_data to a data frame\ndummy_data <- as.data.frame(dummy_data)\n\n# Combine the dummy variables with the original dataset\nheart_data_2_dummies <- bind_cols(heart_data_2, dummy_data)\n\n# Print the updated heart_data\nhead(heart_data_2_dummies, 5)\n\n# A tibble: 5 × 22\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  <dbl> <chr> <chr>             <dbl>       <dbl>     <dbl> <chr>      <dbl>\n1    40 M     ATA                 140         289         0 Normal       172\n2    49 F     NAP                 160         180         0 Normal       156\n3    37 M     ATA                 130         283         0 ST            98\n4    48 F     ASY                 138         214         0 Normal       108\n5    54 M     NAP                 150         195         0 Normal       122\n# ℹ 14 more variables: ExerciseAngina <chr>, Oldpeak <dbl>,\n#   HeartDiseaseFactor <fct>, SexF <dbl>, SexM <dbl>, ExerciseAnginaN <dbl>,\n#   ExerciseAnginaY <dbl>, ChestPainTypeASY <dbl>, ChestPainTypeATA <dbl>,\n#   ChestPainTypeNAP <dbl>, ChestPainTypeTA <dbl>, RestingECGLVH <dbl>,\n#   RestingECGNormal <dbl>, RestingECGST <dbl>\n\n\n\n\nSplit the data\n\n# Set seed\nset.seed(1)  \n\n# Split dummy data and drop original fields\ntrain_index_dummies <- createDataPartition(heart_data_2_dummies$HeartDiseaseFactor, p = 0.8, list = FALSE)\ntrain_data_dummies <- heart_data_2_dummies[train_index_dummies, ]\ntest_data_dummies <- heart_data_2_dummies[-train_index_dummies, ]\ntrain_data_dummies <- train_data_dummies |> select(-Sex, -ExerciseAngina, -ChestPainType, -RestingECG)\ntest_data_dummies <- test_data_dummies |> select(-Sex, -ExerciseAngina, -ChestPainType, -RestingECG)\n\n# Split dummy data and drop original fields\ntrain_index <- createDataPartition(heart_data_2$HeartDiseaseFactor, p = 0.8, list = FALSE)\ntrain_data <- heart_data_2[train_index, ]\ntest_data <- heart_data_2[-train_index, ]\n\n# check results\nnrow(train_data)\n\n[1] 735\n\nnrow(train_data_dummies)\n\n[1] 735\n\nnrow(test_data)\n\n[1] 183\n\nnrow(test_data_dummies)\n\n[1] 183\n\n\n\n\nFit KNN model with Caret\n\n# Fit KNN model, here im trying to define all the options in line instead of as seperate objects to call\nknn_model <- train(\n  HeartDiseaseFactor ~ .,\n  data = train_data_dummies,\n  method = \"knn\",\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n  tuneGrid = expand.grid(k = 1:40),\n  preProcess = c(\"center\", \"scale\")\n)\n\n# Fit summary\nsummary(knn_model)\n\n            Length Class      Mode     \nlearn        2     -none-     list     \nk            1     -none-     numeric  \ntheDots      0     -none-     list     \nxNames      17     -none-     character\nproblemType  1     -none-     character\ntuneValue    1     data.frame list     \nobsLevels    2     -none-     character\nparam        0     -none-     list     \n\n# Generate predictions\ntest_data_dummies$HeartDiseaseKNN <- predict(knn_model, newdata = test_data_dummies)\n\n# Print confusion matrix\nconfusionMatrix(test_data_dummies$HeartDiseaseFactor, test_data_dummies$HeartDiseaseKNN)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 69 13\n         1 18 83\n                                          \n               Accuracy : 0.8306          \n                 95% CI : (0.7683, 0.8819)\n    No Information Rate : 0.5246          \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.6595          \n                                          \n Mcnemar's Test P-Value : 0.4725          \n                                          \n            Sensitivity : 0.7931          \n            Specificity : 0.8646          \n         Pos Pred Value : 0.8415          \n         Neg Pred Value : 0.8218          \n             Prevalence : 0.4754          \n         Detection Rate : 0.3770          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.8288          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nFit logistic regression model with Caret\n\n# Fit logistic model with every predictor\nlogistic_model_1 <- train(\n  HeartDiseaseFactor ~ .,\n  data = train_data,\n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n  preProcess = c(\"center\", \"scale\")\n)\n\n# Fit logistic model with what appear to be strong predictors looking at EDA\nlogistic_model_2 <- train(\n  HeartDiseaseFactor ~ .,\n  data = train_data |> select(HeartDiseaseFactor, ChestPainType, ExerciseAngina, Sex),\n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n  preProcess = c(\"center\", \"scale\")\n)\n\n# Fit logistic model using ExerciseAgina as the single predictor\nlogistic_model_3 <- train(\n  HeartDiseaseFactor ~ .,\n  data = train_data |> select(HeartDiseaseFactor, ExerciseAngina),\n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n  preProcess = c(\"center\", \"scale\")\n)\n\n# Print the training fits for each\n#logistic_model_1 # accuracy 0.8245502\n#logistic_model_2 # accuracy 0.7950438\n#logistic_model_3 # accuracy 0.7266481\n\n# Print summary for best model\nsummary(logistic_model_1)\n\n\nCall:\nNULL\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.9074  -0.5064   0.1892   0.5060   2.6615  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       0.42086    0.11472   3.669 0.000244 ***\nAge               0.17784    0.12891   1.380 0.167702    \nSexM              0.47773    0.11382   4.197 2.70e-05 ***\nChestPainTypeATA -0.77426    0.12643  -6.124 9.13e-10 ***\nChestPainTypeNAP -0.65152    0.10985  -5.931 3.01e-09 ***\nChestPainTypeTA  -0.27786    0.09939  -2.796 0.005181 ** \nRestingBP         0.04619    0.11369   0.406 0.684529    \nCholesterol      -0.44503    0.13050  -3.410 0.000649 ***\nFastingBS         0.49886    0.12226   4.080 4.50e-05 ***\nRestingECGNormal -0.16751    0.14107  -1.187 0.235054    \nRestingECGST     -0.19928    0.14678  -1.358 0.174583    \nMaxHR            -0.45314    0.12928  -3.505 0.000456 ***\nExerciseAnginaY   0.56751    0.12413   4.572 4.83e-06 ***\nOldpeak           0.73112    0.13218   5.531 3.18e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1010.42  on 734  degrees of freedom\nResidual deviance:  546.85  on 721  degrees of freedom\nAIC: 574.85\n\nNumber of Fisher Scoring iterations: 5\n\n# Generate predictions on test data\ntest_data$HeartDiseaseLogistic1 <- predict(logistic_model_1, newdata = test_data)\n\n# Print Conrfusion matrix for test data\nconfusionMatrix(test_data_dummies$HeartDiseaseFactor, test_data$HeartDiseaseLogistic1) # accuracy 0.6612\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 49 33\n         1 29 72\n                                          \n               Accuracy : 0.6612          \n                 95% CI : (0.5877, 0.7294)\n    No Information Rate : 0.5738          \n    P-Value [Acc > NIR] : 0.009685        \n                                          \n                  Kappa : 0.3119          \n                                          \n Mcnemar's Test P-Value : 0.703203        \n                                          \n            Sensitivity : 0.6282          \n            Specificity : 0.6857          \n         Pos Pred Value : 0.5976          \n         Neg Pred Value : 0.7129          \n             Prevalence : 0.4262          \n         Detection Rate : 0.2678          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.6570          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nFit tree models with Caret\n\n# Fit classification tree model with every predictor\ntree_model <- train(\n  HeartDiseaseFactor ~ .,\n  data = train_data,\n  method = \"rpart\",\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n  tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001)),\n  preProcess = c(\"center\", \"scale\")\n)\n\n# Fit random forest model with every predictor\nrandom_forest_model <- train(\n  HeartDiseaseFactor ~ .,\n  data = train_data,\n  method = \"rf\",\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n  tuneGrid = expand.grid(mtry = 1:(ncol(train_data) - 1)),\n  preProcess = c(\"center\", \"scale\")\n)\n\n# Fit boosted model with every predictor\nboosted_tree_model <- train(\n  HeartDiseaseFactor ~ .,\n  data = train_data,\n  method = \"gbm\",\n  trControl = trainControl(method = \"repeatedcv\", number = 10, repeats = 3),\n  tuneGrid = expand.grid(\n    n.trees = c(25, 50, 100, 200),\n    interaction.depth = 1:3,\n    shrinkage = 0.1,\n    n.minobsinnode = 10\n  ),\n  verbose = FALSE,\n  preProcess = c(\"center\", \"scale\")\n)\n\n\n# Generate predictions\ntest_data$HeartDiseaseTree <- predict(tree_model, newdata = test_data)\ntest_data$HeartDiseaseForest <- predict(random_forest_model, newdata = test_data)\ntest_data$HeartDiseaseBoosted <- predict(boosted_tree_model, newdata = test_data)\n\n# Print confusion matrix\nconfusionMatrix(test_data_dummies$HeartDiseaseFactor, test_data$HeartDiseaseTree)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 40 42\n         1 26 75\n                                         \n               Accuracy : 0.6284         \n                 95% CI : (0.554, 0.6986)\n    No Information Rate : 0.6393         \n    P-Value [Acc > NIR] : 0.65203        \n                                         \n                  Kappa : 0.2347         \n                                         \n Mcnemar's Test P-Value : 0.06891        \n                                         \n            Sensitivity : 0.6061         \n            Specificity : 0.6410         \n         Pos Pred Value : 0.4878         \n         Neg Pred Value : 0.7426         \n             Prevalence : 0.3607         \n         Detection Rate : 0.2186         \n   Detection Prevalence : 0.4481         \n      Balanced Accuracy : 0.6235         \n                                         \n       'Positive' Class : 0              \n                                         \n\nconfusionMatrix(test_data_dummies$HeartDiseaseFactor, test_data$HeartDiseaseForest)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 50 32\n         1 30 71\n                                          \n               Accuracy : 0.6612          \n                 95% CI : (0.5877, 0.7294)\n    No Information Rate : 0.5628          \n    P-Value [Acc > NIR] : 0.004221        \n                                          \n                  Kappa : 0.3134          \n                                          \n Mcnemar's Test P-Value : 0.898940        \n                                          \n            Sensitivity : 0.6250          \n            Specificity : 0.6893          \n         Pos Pred Value : 0.6098          \n         Neg Pred Value : 0.7030          \n             Prevalence : 0.4372          \n         Detection Rate : 0.2732          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.6572          \n                                          \n       'Positive' Class : 0               \n                                          \n\nconfusionMatrix(test_data_dummies$HeartDiseaseFactor, test_data$HeartDiseaseBoosted)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 50 32\n         1 33 68\n                                         \n               Accuracy : 0.6448         \n                 95% CI : (0.5708, 0.714)\n    No Information Rate : 0.5464         \n    P-Value [Acc > NIR] : 0.004408       \n                                         \n                  Kappa : 0.2827         \n                                         \n Mcnemar's Test P-Value : 1.000000       \n                                         \n            Sensitivity : 0.6024         \n            Specificity : 0.6800         \n         Pos Pred Value : 0.6098         \n         Neg Pred Value : 0.6733         \n             Prevalence : 0.4536         \n         Detection Rate : 0.2732         \n   Detection Prevalence : 0.4481         \n      Balanced Accuracy : 0.6412         \n                                         \n       'Positive' Class : 0"
  }
]